#WEEK-9
import numpy as np
import matplotlib.pyplot as plt
from keras.layers import Input, Dense
from keras.models import Model
from keras.datasets import mnist

# Load and preprocess the MNIST dataset
(x_train, _), (x_test, _) = mnist.load_data()
x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0
x_train, x_test = x_train.reshape(len(x_train), -1), x_test.reshape(len(x_test), -1)

# Create a simple autoencoder
encoding_dim, input_shape = 32, (784,)
input_layer = Input(shape=input_shape)
encoded = Dense(encoding_dim, activation='relu')(input_layer)
decoded = Dense(784, activation='sigmoid')(encoded)
autoencoder = Model(inputs=input_layer, outputs=decoded)

# Compile and train the autoencoder
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(x_train, x_train, epochs=5)
encoded_imgs = autoencoder.predict(x_test)

# Plot original and decoded images
n = 10  # Number of images to display
for i in range(n * 2):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray') if i < n else plt.imshow(encoded_imgs[i - n].reshape(28, 28), cmap='gray')
    plt.axis('off')
plt.show()



#week-1
import numpy as np 
import tensorflow as tf 
from tensorflow.keras import layers, models 
from tensorflow.keras.datasets import mnist 
(train_images, train_labels), (test_images, test_labels) = mnist.load_data() 
train_images, test_images = train_images / 255.0, test_images / 255.0 
train_labels = tf.keras.utils.to_categorical(train_labels, 10) 
test_labels = tf.keras.utils.to_categorical(test_labels, 10)
model = models.Sequential() 
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) 
model.add(layers.MaxPooling2D((2, 2))) 
model.add(layers.Conv2D(64, (3, 3), activation='relu')) 
model.add(layers.MaxPooling2D((2, 2))) 
model.add(layers.Flatten()) 
model.add(layers.Dense(64, activation='relu')) 
model.add(layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
model.fit(train_images.reshape(-1, 28, 28, 1), train_labels, epochs=5, batch_size=64)
test_loss, test_accuracy = model.evaluate(test_images.reshape(-1, 28, 28, 1), test_labels) 
print("Test accuracy:", test_accuracy)




#week-10
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Dense, Flatten, Reshape
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
# Generate random data for training
def generate_real_samples(n_samples):
    X = np.random.rand(n_samples) * 2 - 1  # Generate random numbers between -1 and 1
    Y = np.ones((n_samples, 1))
    return X, Y
# Generate random noise as input to the generator
def generate_noise(n_samples, noise_dim):
    return np.random.randn(n_samples, noise_dim)
# Define the generator model
def build_generator(noise_dim):
    model = Sequential()
    model.add(Dense(128, input_dim=noise_dim, activation='relu'))
    model.add(Dense(1, activation='linear'))
    return model
# Define the discriminator model
def build_discriminator():
    model = Sequential()
    model.add(Dense(128, input_dim=1, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])
    return model
# Define the GAN model
def build_gan(generator, discriminator):
    discriminator.trainable = False
    model = Sequential()
    model.add(generator)
    model.add(discriminator)
    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))
    return model
# Train the GAN
def train_gan(generator, discriminator, gan, noise_dim, n_epochs, n_batch):
    half_batch = n_batch // 2
    for epoch in range(n_epochs):
        # Train discriminator
        X_real, y_real = generate_real_samples(half_batch)
        d_loss_real = discriminator.train_on_batch(X_real, y_real)

        noise = generate_noise(half_batch, noise_dim)
        X_fake, y_fake = generator.predict(noise), np.zeros((half_batch, 1))
        d_loss_fake = discriminator.train_on_batch(X_fake, y_fake)

        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train generator
        noise = generate_noise(n_batch, noise_dim)
        y_gan = np.ones((n_batch, 1))
        g_loss = gan.train_on_batch(noise, y_gan)

        # Print progress
        print(f"Epoch {epoch + 1}/{n_epochs}, [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}], [G loss: {g_loss}]")
# Parameters
noise_dim = 100
n_epochs = 10000
n_batch = 64

# Build and compile the models
generator = build_generator(noise_dim)
discriminator = build_discriminator()
gan = build_gan(generator, discriminator)
# Train the GAN
train_gan(generator, discriminator, gan, noise_dim, n_epochs, n_batch)
# Generate and plot some fake samples
noise = generate_noise(100, noise_dim)
generated_samples = generator.predict(noise)
plt.hist(generated_samples, bins=50, color='blue', alpha=0.7)
plt.title('Generated Samples')
plt.show()